# LowMemoryLLM

[English Documentation](./README.md)

## é¡¹ç›®æ¦‚è¿°
LowMemoryLLM æ˜¯ä¸€ä¸ªä¸“ä¸ºå†…å­˜å—é™ç¯å¢ƒè®¾è®¡çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œè®­ç»ƒå®ç°ã€‚é€šè¿‡å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæä¾›é«˜æ•ˆçš„æ¨¡å‹æ¨ç†å’Œè®­ç»ƒèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„å†…å­˜å ç”¨ã€‚

## ä¸»è¦ç‰¹æ€§
- ğŸš€ å¤šç§é‡åŒ–é€‰é¡¹ï¼ˆINT8ã€INT4ã€INT2ï¼‰
- ğŸ’¾ æ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œæ”¯æŒç£ç›˜å¸è½½
- ğŸ”„ é«˜æ•ˆçš„æ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶
- ğŸ“¦ é›†æˆ Hugging Face æ¨¡å‹æ”¯æŒ
- ğŸ› ï¸ çµæ´»çš„å†…å­˜ç®¡ç†å’Œä¼˜åŒ–
- ğŸŒ å†…ç½®ä¸‹è½½ç®¡ç†å™¨ï¼Œæ”¯æŒä»£ç†è®¾ç½®
- ğŸ¯ ç¡¬ä»¶æ— å…³çš„è®­ç»ƒæ”¯æŒ
- ğŸ”‹ æ±‡ç¼–ä¼˜åŒ–çš„è®¡ç®—å†…æ ¸

## æŠ€æœ¯ç‰¹æ€§
- é€šè¿‡ç£ç›˜å¸è½½å’Œå†…å­˜æ˜ å°„å®ç°å†…å­˜ä¼˜åŒ–
- å¯é…ç½®çš„é‡åŒ–æ”¯æŒï¼ŒåŒ…æ‹¬æŒ‰é€šé“é‡åŒ–
- KVç¼“å­˜æœºåˆ¶æå‡æ¨ç†æ•ˆç‡
- æ”¯æŒå¤šç§æ¿€æ´»å‡½æ•°ï¼ˆReLUã€GELUã€SILUã€SWISHï¼‰
- é’ˆå¯¹ä½å†…å­˜ç¯å¢ƒä¼˜åŒ–çš„çŸ©é˜µè¿ç®—
- å…¨é¢çš„å¼ é‡æ“ä½œå’Œç®¡ç†
- è·¨å¹³å°è®­ç»ƒæ”¯æŒï¼Œç¡¬ä»¶åŠ é€Ÿä¼˜åŒ–
- å¤šç§ä¼˜åŒ–å™¨å®ç°ï¼ˆSGDã€Adamã€AdamWã€RMSpropï¼‰
- æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
- æ¢¯åº¦è£å‰ªå’Œå½’ä¸€åŒ–

## ç³»ç»Ÿè¦æ±‚
- æ”¯æŒ C11 çš„ C ç¼–è¯‘å™¨
- CMake æ„å»ºç³»ç»Ÿ
- è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ç”¨äºæ¨¡å‹æƒé‡å’Œäº¤æ¢æ–‡ä»¶
- ç½‘ç»œè¿æ¥ç”¨äºæ¨¡å‹ä¸‹è½½
- ï¼ˆå¯é€‰ï¼‰æ”¯æŒ AVX2/NEON çš„ç¡¬ä»¶åŠ é€Ÿ

## å®‰è£…æ–¹æ³•
```bash
git clone https://github.com/2404589803/LowMemoryLLM.git
cd LowMemoryLLM
mkdir build && cd build
cmake ..
make
```

## ä½¿ç”¨æ–¹æ³•

### æ¨ç†
1. é…ç½®æ¨¡å‹è®¾ç½®ï¼š
```c
LLMConfig config = {
    .vocab_size = 50257,
    .hidden_size = 768,
    .num_layers = 12,
    .max_seq_length = 1024,
    .batch_size = 1,
    .use_cache = 1
};
```

2. åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨ï¼š
```c
MemoryManager mem_manager = {
    .use_disk_offload = 1,
    .use_memory_map = 1,
    .prefetch_size = 1024 * 1024
};
```

3. åˆå§‹åŒ–å¹¶è¿è¡Œæ¨ç†ï¼š
```c
llm_init(&config, &mem_manager);
llm_load_weights("path/to/weights");
// è¿è¡Œæ¨ç†...
```

### è®­ç»ƒ
1. é…ç½®è®­ç»ƒå‚æ•°ï¼š
```c
TrainingConfig train_config = {
    .batch_size = 32,
    .num_epochs = 100,
    .loss_type = LOSS_CROSS_ENTROPY,
    .optimizer = {
        .type = OPTIMIZER_ADAM,
        .learning_rate = 0.001f,
        .beta1 = 0.9f,
        .beta2 = 0.999f,
        .epsilon = 1e-8f
    },
    .gradient_clip_norm = 1.0f,
    .enable_mixed_precision = 1
};
```

2. åˆå§‹åŒ–è®­ç»ƒç³»ç»Ÿï¼š
```c
TrainingExtension extension = {
    .backward_matrix_multiply = backward_matrix_multiply_asm,
    .backward_vector_add = backward_vector_add_asm,
    // è®¾ç½®å…¶ä»–å‡½æ•°æŒ‡é’ˆ...
};

training_init(device, &extension);
training_configure(&train_config);
```

3. è®­ç»ƒå¾ªç¯ï¼š
```c
TrainingState state = {0};
TrainingCallbacks callbacks = {
    .on_epoch_begin = my_epoch_begin_callback,
    .on_batch_end = my_batch_end_callback
};

for (size_t epoch = 0; epoch < train_config.num_epochs; epoch++) {
    for (size_t batch = 0; batch < num_batches; batch++) {
        void* inputs = prepare_batch_inputs(batch);
        void* targets = prepare_batch_targets(batch);
        training_step(model, inputs, targets, &state, &callbacks);
    }
    
    float metrics[2];
    training_evaluate(model, val_inputs, val_targets, metrics, 2);
}
```

## ç¡¬ä»¶æ”¯æŒ
- x86_64 æ¶æ„ï¼Œæ”¯æŒ AVX/AVX2 ä¼˜åŒ–
- ARM64 æ¶æ„ï¼Œæ”¯æŒ NEON ä¼˜åŒ–
- é€šç”¨ CPU å®ç°
- å¯æ‰©å±•çš„è®¾å¤‡æŠ½è±¡å±‚

## å¼€æºåè®®
æœ¬é¡¹ç›®é‡‡ç”¨ MIT å¼€æºåè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è´¡çŒ®æŒ‡å—
æ¬¢è¿æäº¤ Pull Request æ¥å¸®åŠ©æ”¹è¿›é¡¹ç›®ï¼

## è”ç³»æ–¹å¼
[åœ¨æ­¤æ·»åŠ è”ç³»æ–¹å¼] 